{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - Parallel Data Processing with Task Dependencies\n",
    "\n",
    "**GOAL:** The goal of this exercise is to show how to pass object IDs into remote functions to encode dependencies between tasks.\n",
    "\n",
    "In this exercise, we construct a sequence of tasks each of which depends on the previous mimicking a data parallel application. Within each sequence, tasks are executed serially, but multiple sequences can be executed in parallel.\n",
    "\n",
    "In this exercise, you will use Ray to parallelize the computation below and speed it up.\n",
    "\n",
    "### Concept for this Exercise - Task Dependencies\n",
    "\n",
    "Suppose we have two remote functions defined as follows.\n",
    "\n",
    "```python\n",
    "@ray.remote\n",
    "def f(x):\n",
    "    return x\n",
    "```\n",
    "\n",
    "Arguments can be passed into remote functions as usual.\n",
    "\n",
    "```python\n",
    ">>> x1_id = f.remote(1)\n",
    ">>> ray.get(x1_id)\n",
    "1\n",
    "\n",
    ">>> x2_id = f.remote([1, 2, 3])\n",
    ">>> ray.get(x2_id)\n",
    "[1, 2, 3]\n",
    "```\n",
    "\n",
    "**Object IDs** can also be passed into remote functions. When the function actually gets executed, **the argument will be a retrieved as a regular Python object**.\n",
    "\n",
    "```python\n",
    ">>> y1_id = f.remote(x1_id)\n",
    ">>> ray.get(y1_id)\n",
    "1\n",
    "\n",
    ">>> y2_id = f.remote(x2_id)\n",
    ">>> ray.get(y2_id)\n",
    "[1, 2, 3]\n",
    "```\n",
    "\n",
    "So when implementing a remote function, the function should expect a regular Python object regardless of whether the caller passes in a regular Python object or an object ID.\n",
    "\n",
    "**Task dependencies affect scheduling.** In the example above, the task that creates `y1_id` depends on the task that creates `x1_id`. This has the following implications.\n",
    "\n",
    "- The second task will not be executed until the first task has finished executing.\n",
    "- If the two tasks are scheduled on different machines, the output of the first task (the value corresponding to `x1_id`) will be copied over the network to the machine where the second task is scheduled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import ray\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-01 17:27:12,828\tINFO resource_spec.py:205 -- Starting Ray with 1.07 GiB memory available for workers and up to 0.55 GiB for objects. You can adjust these settings with ray.remote(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2021-02-01 17:27:13,131\tWARNING services.py:1237 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '10.0.10.120',\n",
       " 'redis_address': '10.0.10.120:46073',\n",
       " 'object_store_address': '/tmp/ray/session_2021-02-01_17-27-12_827271_601/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-02-01_17-27-12_827271_601/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2021-02-01_17-27-12_827271_601'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(num_cpus=4, include_webui=False, ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some helper functions that mimic an example pattern of a data parallel application.\n",
    "\n",
    "**EXERCISE:** You will need to turn all of these functions into remote functions. When you turn these functions into remote function, you do not have to worry about whether the caller passes in an object ID or a regular object. In both cases, the arguments will be regular objects when the function executes. This means that even if you pass in an object ID, you **do not need to call `ray.get`** inside of these remote functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def load_data(filename):\n",
    "    time.sleep(0.1)\n",
    "    return np.ones((1000, 100))\n",
    "\n",
    "@ray.remote\n",
    "def normalize_data(data):\n",
    "    time.sleep(0.1)\n",
    "    return data - np.mean(data, axis=0)\n",
    "\n",
    "@ray.remote\n",
    "def extract_features(normalized_data):\n",
    "    time.sleep(0.1)\n",
    "    return np.hstack([normalized_data, normalized_data ** 2])\n",
    "\n",
    "@ray.remote\n",
    "def compute_loss(features):\n",
    "    num_data, dim = features.shape\n",
    "    time.sleep(0.1)\n",
    "    return np.sum((np.dot(features, np.ones(dim)) - np.ones(num_data)) ** 2)\n",
    "\n",
    "assert hasattr(load_data, 'remote'), 'load_data must be a remote function'\n",
    "assert hasattr(normalize_data, 'remote'), 'normalize_data must be a remote function'\n",
    "assert hasattr(extract_features, 'remote'), 'extract_features must be a remote function'\n",
    "assert hasattr(compute_loss, 'remote'), 'compute_loss must be a remote function'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** The loop below takes too long. Parallelize the four passes through the loop by turning `load_data`, `normalize_data`, `extract_features`, and `compute_loss` into remote functions and then retrieving the losses with `ray.get`.\n",
    "\n",
    "**NOTE:** You should only use **ONE** call to `ray.get`. For example, the object ID returned by `load_data` should be passed directly into `normalize_data` without needing to be retrieved by the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The losses are [ObjectID(c2a79d56f2feffffffff0100000000c001000000), ObjectID(f0f55112a837ffffffff0100000000c001000000), ObjectID(15835de56970ffffffff0100000000c001000000), ObjectID(604166a15dd4ffffffff0100000000c001000000)].\n",
      "\n",
      "The loss is 4000.0. This took 0.7030258178710938 seconds. Run the next cell to see if the exercise was done correctly.\n"
     ]
    }
   ],
   "source": [
    "# Sleep a little to improve the accuracy of the timing measurements below.\n",
    "time.sleep(2.0)\n",
    "start_time = time.time()\n",
    "\n",
    "losses = []\n",
    "for filename in ['file1', 'file2', 'file3', 'file4']:\n",
    "    inner_start = time.time()\n",
    "\n",
    "    data = load_data.remote(filename)\n",
    "    normalized_data = normalize_data.remote(data)\n",
    "    features = extract_features.remote(normalized_data)\n",
    "    loss = compute_loss.remote(features)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    inner_end = time.time()\n",
    "    \n",
    "    if inner_end - inner_start >= 0.1:\n",
    "        raise Exception('You may be calling ray.get inside of the for loop! '\n",
    "                        'Doing this will prevent parallelism from being exposed. '\n",
    "                        'Make sure to only call ray.get once outside of the for loop.')\n",
    "\n",
    "print('The losses are {}.'.format(losses) + '\\n')\n",
    "loss = sum(ray.get(losses))\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print('The loss is {}. This took {} seconds. Run the next cell to see '\n",
    "      'if the exercise was done correctly.'.format(loss, duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VERIFY:** Run some checks to verify that the changes you made to the code were correct. Some of the checks should fail when you initially run the cells. After completing the exercises, the checks should pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! The example took 0.7030258178710938 seconds.\n"
     ]
    }
   ],
   "source": [
    "assert loss == 4000\n",
    "assert duration < 0.8, ('The loop took {} seconds. This is too slow.'\n",
    "                        .format(duration))\n",
    "assert duration > 0.4, ('The loop took {} seconds. This is too fast.'\n",
    "                        .format(duration))\n",
    "\n",
    "print('Success! The example took {} seconds.'.format(duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Use the UI to view the task timeline and to verify that the four tasks were executed in parallel. You can do this as follows.\n",
    "\n",
    "1. Run the following cell to generate a JSON file containing the profiling data.\n",
    "2. Download the timeline file by right clicking on `timeline02.json` in the navigator to the left and choosing **\"Download\"**.\n",
    "3. Open [chrome://tracing/](chrome://tracing/) in the Chrome web browser, click on the **\"Load\"** button and load the downloaded JSON file.\n",
    "\n",
    "To navigate within the timeline, do the following.\n",
    "- Move around by clicking and dragging.\n",
    "- Zoom in and out by holding **alt** and scrolling.\n",
    "\n",
    "**NOTE:** The timeline visualization will only work in **Chrome**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.timeline(filename=\"timeline02.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask[complete] in /srv/conda/envs/notebook/lib/python3.6/site-packages (2021.1.1)\n",
      "Requirement already satisfied: pyyaml in /srv/conda/envs/notebook/lib/python3.6/site-packages (from dask[complete]) (5.4.1)\n",
      "Collecting toolz>=0.8.2; extra == \"complete\"\n",
      "  Downloading toolz-0.11.1-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 2.5 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.25.0; extra == \"complete\" in /srv/conda/envs/notebook/lib/python3.6/site-packages (from dask[complete]) (1.1.5)\n",
      "Collecting fsspec>=0.6.0; extra == \"complete\"\n",
      "  Downloading fsspec-0.8.5-py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 4.4 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting distributed>=2.0; extra == \"complete\"\n",
      "  Downloading distributed-2021.1.1-py3-none-any.whl (672 kB)\n",
      "\u001b[K     |████████████████████████████████| 672 kB 8.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting partd>=0.3.10; extra == \"complete\"\n",
      "  Downloading partd-1.1.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: bokeh!=2.0.0,>=1.0.0; extra == \"complete\" in /srv/conda/envs/notebook/lib/python3.6/site-packages (from dask[complete]) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.15.1; extra == \"complete\" in /srv/conda/envs/notebook/lib/python3.6/site-packages (from dask[complete]) (1.19.4)\n",
      "Requirement already satisfied: cloudpickle>=0.2.2; extra == \"complete\" in /srv/conda/envs/notebook/lib/python3.6/site-packages (from dask[complete]) (1.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from pandas>=0.25.0; extra == \"complete\"->dask[complete]) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from pandas>=0.25.0; extra == \"complete\"->dask[complete]) (2021.1)\n",
      "Collecting msgpack>=0.6.0\n",
      "  Downloading msgpack-1.0.2-cp36-cp36m-manylinux1_x86_64.whl (272 kB)\n",
      "\u001b[K     |████████████████████████████████| 272 kB 11.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.6/site-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]) (49.2.0.post20200712)\n",
      "Collecting sortedcontainers!=2.0.0,!=2.0.1\n",
      "  Downloading sortedcontainers-2.3.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting tblib>=1.6.0\n",
      "  Downloading tblib-1.7.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting zict>=0.1.3\n",
      "  Downloading zict-2.0.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: tornado>=5; python_version < \"3.8\" in /srv/conda/envs/notebook/lib/python3.6/site-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]) (6.0.4)\n",
      "Requirement already satisfied: click>=6.6 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]) (7.1.2)\n",
      "Requirement already satisfied: psutil>=5.0 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]) (5.7.2)\n",
      "Requirement already satisfied: contextvars; python_version < \"3.7\" in /srv/conda/envs/notebook/lib/python3.6/site-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]) (2.4)\n",
      "Collecting locket\n",
      "  Downloading locket-0.2.1-py2.py3-none-any.whl (4.1 kB)\n",
      "Requirement already satisfied: Jinja2>=2.7 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from bokeh!=2.0.0,>=1.0.0; extra == \"complete\"->dask[complete]) (2.11.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from bokeh!=2.0.0,>=1.0.0; extra == \"complete\"->dask[complete]) (3.7.4.3)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from bokeh!=2.0.0,>=1.0.0; extra == \"complete\"->dask[complete]) (8.1.0)\n",
      "Requirement already satisfied: packaging>=16.8 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from bokeh!=2.0.0,>=1.0.0; extra == \"complete\"->dask[complete]) (20.4)\n",
      "Requirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas>=0.25.0; extra == \"complete\"->dask[complete]) (1.15.0)\n",
      "Collecting heapdict\n",
      "  Downloading HeapDict-1.0.1-py3-none-any.whl (3.9 kB)\n",
      "Requirement already satisfied: immutables>=0.9 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from contextvars; python_version < \"3.7\"->distributed>=2.0; extra == \"complete\"->dask[complete]) (0.14)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from Jinja2>=2.7->bokeh!=2.0.0,>=1.0.0; extra == \"complete\"->dask[complete]) (1.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from packaging>=16.8->bokeh!=2.0.0,>=1.0.0; extra == \"complete\"->dask[complete]) (2.4.7)\n",
      "Installing collected packages: toolz, fsspec, msgpack, sortedcontainers, tblib, heapdict, zict, distributed, locket, partd\n",
      "Successfully installed distributed-2021.1.1 fsspec-0.8.5 heapdict-1.0.1 locket-0.2.1 msgpack-1.0.2 partd-1.1.0 sortedcontainers-2.3.0 tblib-1.7.0 toolz-0.11.1 zict-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install dask[complete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client, LocalCluster\n",
    "\n",
    "client = Client(LocalCluster(n_workers=4, processes=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "@dask.delayed\n",
    "def load_data(filename):\n",
    "    time.sleep(0.1)\n",
    "    return np.ones((1000, 100))\n",
    "\n",
    "@dask.delayed\n",
    "def normalize_data(data):\n",
    "    time.sleep(0.1)\n",
    "    return data - np.mean(data, axis=0)\n",
    "\n",
    "@dask.delayed\n",
    "def extract_features(normalized_data):\n",
    "    time.sleep(0.1)\n",
    "    return np.hstack([normalized_data, normalized_data ** 2])\n",
    "\n",
    "@dask.delayed\n",
    "def compute_loss(features):\n",
    "    num_data, dim = features.shape\n",
    "    time.sleep(0.1)\n",
    "    return np.sum((np.dot(features, np.ones(dim)) - np.ones(num_data)) ** 2)\n",
    "\n",
    "assert hasattr(load_data, 'remote'), 'load_data must be a remote function'\n",
    "assert hasattr(normalize_data, 'remote'), 'normalize_data must be a remote function'\n",
    "assert hasattr(extract_features, 'remote'), 'extract_features must be a remote function'\n",
    "assert hasattr(compute_loss, 'remote'), 'compute_loss must be a remote function'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The losses are [Delayed('compute_loss-f0ea7db4-5b79-420b-ae10-e359f730924e'), Delayed('compute_loss-88162b97-9f6c-4953-b1b5-be04c9eb0871'), Delayed('compute_loss-a4e28890-ee11-4d76-8d61-629f2fbdf7f0'), Delayed('compute_loss-b006a3a1-2251-46b7-9c15-2d3ddde64849')].\n",
      "\n",
      "The loss is 4000.0. This took 0.574378252029419 seconds. Run the next cell to see if the exercise was done correctly.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "losses = []\n",
    "for filename in ['file1', 'file2', 'file3', 'file4']:\n",
    "    inner_start = time.time()\n",
    "\n",
    "    data = load_data(filename)\n",
    "    normalized_data = normalize_data(data)\n",
    "    features = extract_features(normalized_data)\n",
    "    loss = compute_loss(features)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    inner_end = time.time()\n",
    "    \n",
    "    if inner_end - inner_start >= 0.1:\n",
    "        raise Exception('You may be calling ray.get inside of the for loop! '\n",
    "                        'Doing this will prevent parallelism from being exposed. '\n",
    "                        'Make sure to only call ray.get once outside of the for loop.')\n",
    "\n",
    "print('The losses are {}.'.format(losses) + '\\n')\n",
    "loss = dask.compute(sum(losses))[0]\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print('The loss is {}. This took {} seconds. Run the next cell to see '\n",
    "      'if the exercise was done correctly.'.format(loss, duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! The example took 0.574378252029419 seconds.\n"
     ]
    }
   ],
   "source": [
    "assert loss == 4000\n",
    "assert duration < 0.8, ('The loop took {} seconds. This is too slow.'\n",
    "                        .format(duration))\n",
    "assert duration > 0.4, ('The loop took {} seconds. This is too fast.'\n",
    "                        .format(duration))\n",
    "\n",
    "print('Success! The example took {} seconds.'.format(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application: Parallel web-scraping\n",
    "\n",
    "One useful application of what we have learned so far is to scrape information from the web. We will illustrate this in a toy setting, but the same principles apply on a large scale where crawling through websites, parsing them and extracting useful content (e.g. for building a search index or populating a database) is often very computationally demanding.\n",
    "\n",
    "We break up the process into multiple steps. We first grab the raw HTML of the website using Python's requests package. Then, we use BeautifulSoup to parse the HTML to find the relevant information. Finally, we populate a pandas DataFrames so that we are able to work with the data.\n",
    "\n",
    "To demonstrate this, we scrape GitHub commits to see the latest commits on several repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /srv/conda/envs/notebook/lib/python3.6/site-packages (2.24.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from requests) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: bs4 in /srv/conda/envs/notebook/lib/python3.6/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in /srv/conda/envs/notebook/lib/python3.6/site-packages (from beautifulsoup4->bs4) (2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-01 17:50:29,049\tINFO resource_spec.py:205 -- Starting Ray with 1.03 GiB memory available for workers and up to 0.51 GiB for objects. You can adjust these settings with ray.remote(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2021-02-01 17:50:29,344\tWARNING services.py:1237 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '10.0.10.120',\n",
       " 'redis_address': '10.0.10.120:26784',\n",
       " 'object_store_address': '/tmp/ray/session_2021-02-01_17-50-29_048543_1890/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-02-01_17-50-29_048543_1890/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2021-02-01_17-50-29_048543_1890'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import ray\n",
    "import requests\n",
    "import time\n",
    "\n",
    "ray.init(num_cpus=4, ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function uses these libraries to parse the latest commits from several repositories on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def fetch_commits(repo):\n",
    "    url = 'https://github.com/{}/commits/master'.format(repo)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    results = []\n",
    "    for commit_elt in soup.find_all('li', class_='commit'):\n",
    "        title = commit_elt.find_all('a', class_='message')[0].attrs.get('aria-label').split('\\n')[0]\n",
    "        link_elts = commit_elt.find_all('a', class_='issue-link')\n",
    "        link = None\n",
    "        for le in link_elts:\n",
    "            if 'issue' in le.attrs['href'].lower():\n",
    "                link = le.attrs['href']\n",
    "        results.append(dict(title=title, link=link))\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df['repository'] = repo\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this out to get results for some ray related topics serially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing the dataframe took 10.213820934295654 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "repos = [\"ray-project/ray\", \"modin-project/modin\", \"tensorflow/tensorflow\", \"apache/arrow\"]\n",
    "results = []\n",
    "for repo in repos:\n",
    "    df = fetch_commits.remote(repo)\n",
    "    results.append(df)\n",
    "\n",
    "results = ray.get(results)\n",
    "df = pd.concat(results, sort=False)\n",
    "duration = time.time() - start\n",
    "print(\"Constructing the dataframe took {} seconds.\".format(duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**: Speed up the above serial query by making `fetch_commits` a remote function in order to scrape GitHub results in parallel. Then, see a sample of the data scraped below and feel free to play with the data to find other resources to learn more about these libraries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repository</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [repository]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import dask\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "@dask.delayed\n",
    "def fetch_commits(repo):\n",
    "    url = 'https://github.com/{}/commits/master'.format(repo)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    results = []\n",
    "    for commit_elt in soup.find_all('li', class_='commit'):\n",
    "        title = commit_elt.find_all('a', class_='message')[0].attrs.get('aria-label').split('\\n')[0]\n",
    "        link_elts = commit_elt.find_all('a', class_='issue-link')\n",
    "        link = None\n",
    "        for le in link_elts:\n",
    "            if 'issue' in le.attrs['href'].lower():\n",
    "                link = le.attrs['href']\n",
    "        results.append(dict(title=title, link=link))\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df['repository'] = repo\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing the dataframe took 4.518927574157715 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "repos = [\"ray-project/ray\", \"modin-project/modin\", \"tensorflow/tensorflow\", \"apache/arrow\"]\n",
    "results = []\n",
    "for repo in repos:\n",
    "    df = fetch_commits(repo)\n",
    "    results.append(df)\n",
    "\n",
    "results = dask.compute(*results)\n",
    "df = pd.concat(results, sort=False)\n",
    "duration = time.time() - start\n",
    "print(\"Constructing the dataframe took {} seconds.\".format(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repository</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [repository]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
